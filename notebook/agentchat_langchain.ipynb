{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ae1f50ec",
      "metadata": {
        "id": "ae1f50ec"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_function_call.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9a71fa36",
      "metadata": {
        "id": "9a71fa36"
      },
      "source": [
        "# Auto Generated Agent Chat: Task Solving with Langchain Provided Tools as Functions\n",
        "\n",
        "AutoGen offers conversable agents powered by LLM, tool, or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participants through multi-agent conversation. Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
        "\n",
        "In this notebook, we demonstrate how to use `AssistantAgent` and `UserProxyAgent` to make function calls with the new feature of OpenAI models (in model version 0613) with a set of Langchain-provided tools and toolkits, to demonstrate how to leverage the 35+ tools available. \n",
        "A specified prompt and function configs must be passed to `AssistantAgent` to initialize the agent. The corresponding functions must be passed to `UserProxyAgent`, which will execute any function calls made by `AssistantAgent`. Besides this requirement of matching descriptions with functions, we recommend checking the system message in the `AssistantAgent` to ensure the instructions align with the function call descriptions.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "AutoGen requires `Python>=3.8`. To run this notebook example, please install the [mathchat] option since we will import functions from `MathUserProxyAgent`:\n",
        "```bash\n",
        "pip install \"pyautogen[mathchat]\"\n",
        "```\n",
        "```bash\n",
        "%pip install Langchain\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2b803c17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b803c17",
        "outputId": "2e12aa3f-e46c-4b82-cc2e-1495f70a2961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pyautogen~=0.1.0 in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen[mathchat]~=0.1.0) (0.1.13)\n",
            "Requirement already satisfied: openai<1 in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (0.28.1)\n",
            "Requirement already satisfied: diskcache in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (5.6.3)\n",
            "Requirement already satisfied: termcolor in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2.3.0)\n",
            "Requirement already satisfied: flaml in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2.1.1)\n",
            "Requirement already satisfied: python-dotenv in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.0.0)\n",
            "Requirement already satisfied: sympy in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen[mathchat]~=0.1.0) (1.12)\n",
            "Requirement already satisfied: pydantic==1.10.9 in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen[mathchat]~=0.1.0) (1.10.9)\n",
            "Requirement already satisfied: wolframalpha in /home/vscode/.local/lib/python3.10/site-packages (from pyautogen[mathchat]~=0.1.0) (5.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /home/vscode/.local/lib/python3.10/site-packages (from pydantic==1.10.9->pyautogen[mathchat]~=0.1.0) (4.8.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/site-packages (from openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /home/vscode/.local/lib/python3.10/site-packages (from openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /home/vscode/.local/lib/python3.10/site-packages (from openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (3.8.6)\n",
            "Requirement already satisfied: NumPy>=1.17.0rc1 in /home/vscode/.local/lib/python3.10/site-packages (from flaml->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.26.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/vscode/.local/lib/python3.10/site-packages (from sympy->pyautogen[mathchat]~=0.1.0) (1.3.0)\n",
            "Requirement already satisfied: xmltodict in /home/vscode/.local/lib/python3.10/site-packages (from wolframalpha->pyautogen[mathchat]~=0.1.0) (0.13.0)\n",
            "Requirement already satisfied: more-itertools in /home/vscode/.local/lib/python3.10/site-packages (from wolframalpha->pyautogen[mathchat]~=0.1.0) (10.1.0)\n",
            "Requirement already satisfied: jaraco.context in /home/vscode/.local/lib/python3.10/site-packages (from wolframalpha->pyautogen[mathchat]~=0.1.0) (4.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install \"pyautogen[mathchat]~=0.1.0\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5ebd2397",
      "metadata": {
        "id": "5ebd2397"
      },
      "source": [
        "## Set your API Endpoint\n",
        "\n",
        "The [`config_list_from_models`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_models) function tries to create a list of configurations using Azure OpenAI endpoints and OpenAI endpoints for the provided list of models. It assumes the api keys and api bases are stored in the corresponding environment variables or local txt files:\n",
        "\n",
        "- OpenAI API key: os.environ[\"OPENAI_API_KEY\"] or `openai_api_key_file=\"key_openai.txt\"`.\n",
        "- Azure OpenAI API key: os.environ[\"AZURE_OPENAI_API_KEY\"] or `aoai_api_key_file=\"key_aoai.txt\"`. Multiple keys can be stored, one per line.\n",
        "- Azure OpenAI API base: os.environ[\"AZURE_OPENAI_API_BASE\"] or `aoai_api_base_file=\"base_aoai.txt\"`. Multiple bases can be stored, one per line.\n",
        "\n",
        "It's OK to have only the OpenAI API key, or only the Azure OpenAI API key + base.\n",
        "If you open this notebook in google colab, you can upload your files by clicking the file icon on the left panel and then choosing \"upload file\" icon.\n",
        "\n",
        "The following code excludes Azure OpenAI endpoints from the config list because some endpoints don't support functions yet. Remove the `exclude` argument if they do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "dca301a4",
      "metadata": {
        "id": "dca301a4"
      },
      "outputs": [],
      "source": [
        "import autogen\n",
        "\n",
        "config_list = autogen.config_list_from_models(model_list=[\"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"], exclude=\"aoai\", openai_api_key_file=\"key_openai.txt\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2b9526e7",
      "metadata": {
        "id": "2b9526e7"
      },
      "source": [
        "## Making Function Calls\n",
        "\n",
        "In this example, we demonstrate function call execution with `AssistantAgent` and `UserProxyAgent`. With the default system prompt of `AssistantAgent`, we allow the LLM assistant to perform tasks with code, and the `UserProxyAgent` would extract code blocks from the LLM response and execute them. With the new \"function_call\" feature, we define functions and specify the description of the function in the OpenAI config for the `AssistantAgent`. Then we register the functions in `UserProxyAgent`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ngpFkWh5ad9M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngpFkWh5ad9M",
        "outputId": "d33b51c6-5dfe-402d-dce4-6b40fca85d3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting Langchain\n",
            "  Downloading langchain-0.0.322-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/site-packages (from Langchain) (6.0.1)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from Langchain)\n",
            "  Downloading SQLAlchemy-2.0.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/vscode/.local/lib/python3.10/site-packages (from Langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /home/vscode/.local/lib/python3.10/site-packages (from Langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/vscode/.local/lib/python3.10/site-packages (from Langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from Langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from Langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.43 (from Langchain)\n",
            "  Downloading langsmith-0.0.51-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /home/vscode/.local/lib/python3.10/site-packages (from Langchain) (1.26.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /home/vscode/.local/lib/python3.10/site-packages (from Langchain) (1.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/site-packages (from Langchain) (2.31.0)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from Langchain)\n",
            "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/vscode/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->Langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/site-packages (from anyio<4.0->Langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/vscode/.local/lib/python3.10/site-packages (from anyio<4.0->Langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /home/vscode/.local/lib/python3.10/site-packages (from anyio<4.0->Langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->Langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->Langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->Langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /home/vscode/.local/lib/python3.10/site-packages (from pydantic<3,>=1->Langchain) (4.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->Langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2->Langchain) (2023.7.22)\n",
            "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->Langchain)\n",
            "  Downloading greenlet-3.0.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->Langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->Langchain) (1.0.0)\n",
            "Downloading langchain-0.0.322-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.0.51-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading greenlet-3.0.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (612 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.9/612.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Installing collected packages: typing-inspect, tenacity, marshmallow, jsonpointer, greenlet, SQLAlchemy, langsmith, jsonpatch, dataclasses-json, Langchain\n",
            "Successfully installed Langchain-0.0.322 SQLAlchemy-2.0.22 dataclasses-json-0.6.1 greenlet-3.0.0 jsonpatch-1.33 jsonpointer-2.4 langsmith-0.0.51 marshmallow-3.20.1 tenacity-8.2.3 typing-inspect-0.9.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "qCzNbbVajvpc",
      "metadata": {
        "id": "qCzNbbVajvpc"
      },
      "outputs": [],
      "source": [
        "# Import things that are needed generically\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool\n",
        "from typing import Optional, Type\n",
        "import math\n",
        "\n",
        "class CircumferenceToolInput(BaseModel):\n",
        "    radius: float = Field()\n",
        "\n",
        "class CircumferenceTool(BaseTool):\n",
        "    name = \"circumference_calculator\"\n",
        "    description = \"Use this tool when you need to calculate a circumference using the radius of a circle\"\n",
        "    args_schema: Type[BaseModel] = CircumferenceToolInput\n",
        "\n",
        "    def _run(self, radius: float):\n",
        "        return float(radius) * 2.0 * math.pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "COlL5_98atDs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COlL5_98atDs",
        "outputId": "24ce236d-8993-4a69-99e2-65453574d61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
            "\n",
            "Read the file with the path 'Test.txt', then calculate the circumference of a circle that has a radius of that files contents.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "[autogen.oai.completion: 10-25 02:57:20] {788} WARNING - Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:autogen.oai.completion:Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n"
          ]
        },
        {
          "ename": "AuthenticationError",
          "evalue": "No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/autogen/notebook/agentchat_langchain.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m user_proxy\u001b[39m.\u001b[39mregister_function(\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     function_map\u001b[39m=\u001b[39m{\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m         custom_tool\u001b[39m.\u001b[39mname: custom_tool\u001b[39m.\u001b[39m_run,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m         read_file_tool\u001b[39m.\u001b[39mname: read_file_tool\u001b[39m.\u001b[39m_run,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m chatbot \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mAssistantAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchatbot\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     system_message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFor coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     llm_config\u001b[39m=\u001b[39mllm_config,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m user_proxy\u001b[39m.\u001b[39;49minitiate_chat(\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     chatbot,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mRead the file with the path \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTest.txt\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m, then calculate the circumference of a circle that has a radius of that files contents.\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m#\" 7.81mm\" in the file\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m     llm_config\u001b[39m=\u001b[39;49mllm_config,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/agentchat_langchain.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m )\n",
            "File \u001b[0;32m/workspaces/autogen/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
            "File \u001b[0;32m/workspaces/autogen/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
            "File \u001b[0;32m/workspaces/autogen/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
            "File \u001b[0;32m/workspaces/autogen/autogen/agentchat/conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 781\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    783\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
            "File \u001b[0;32m/workspaces/autogen/autogen/agentchat/conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    603\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    605\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    607\u001b[0m     context\u001b[39m=\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_oai_system_message \u001b[39m+\u001b[39;49m messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mllm_config\n\u001b[1;32m    608\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
            "File \u001b[0;32m/workspaces/autogen/autogen/oai/completion.py:834\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39mCache(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[1;32m    833\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(seed)\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout)\n",
            "File \u001b[0;32m/workspaces/autogen/autogen/oai/completion.py:222\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config:\n\u001b[0;32m--> 222\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n\u001b[1;32m    223\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39mcreate(request_timeout\u001b[39m=\u001b[39mrequest_timeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:151\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[1;32m    143\u001b[0m         timeout,\n\u001b[1;32m    144\u001b[0m         stream,\n\u001b[1;32m    145\u001b[0m         headers,\n\u001b[1;32m    146\u001b[0m         request_timeout,\n\u001b[1;32m    147\u001b[0m         typed_api_type,\n\u001b[1;32m    148\u001b[0m         requestor,\n\u001b[1;32m    149\u001b[0m         url,\n\u001b[1;32m    150\u001b[0m         params,\n\u001b[0;32m--> 151\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__prepare_create_request(\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[1;32m    155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:108\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[0;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    106\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m MAX_TIMEOUT\n\u001b[0;32m--> 108\u001b[0m requestor \u001b[39m=\u001b[39m api_requestor\u001b[39m.\u001b[39;49mAPIRequestor(\n\u001b[1;32m    109\u001b[0m     api_key,\n\u001b[1;32m    110\u001b[0m     api_base\u001b[39m=\u001b[39;49mapi_base,\n\u001b[1;32m    111\u001b[0m     api_type\u001b[39m=\u001b[39;49mapi_type,\n\u001b[1;32m    112\u001b[0m     api_version\u001b[39m=\u001b[39;49mapi_version,\n\u001b[1;32m    113\u001b[0m     organization\u001b[39m=\u001b[39;49morganization,\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    115\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mclass_url(engine, api_type, api_version)\n\u001b[1;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    117\u001b[0m     deployment_id,\n\u001b[1;32m    118\u001b[0m     engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m     params,\n\u001b[1;32m    127\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:139\u001b[0m, in \u001b[0;36mAPIRequestor.__init__\u001b[0;34m(self, key, api_base, api_type, api_version, organization)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    132\u001b[0m     key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     organization\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_base \u001b[39m=\u001b[39m api_base \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_base\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key \u001b[39m=\u001b[39m key \u001b[39mor\u001b[39;00m util\u001b[39m.\u001b[39;49mdefault_api_key()\n\u001b[1;32m    140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_type \u001b[39m=\u001b[39m (\n\u001b[1;32m    141\u001b[0m         ApiType\u001b[39m.\u001b[39mfrom_str(api_type)\n\u001b[1;32m    142\u001b[0m         \u001b[39mif\u001b[39;00m api_type\n\u001b[1;32m    143\u001b[0m         \u001b[39melse\u001b[39;00m ApiType\u001b[39m.\u001b[39mfrom_str(openai\u001b[39m.\u001b[39mapi_type)\n\u001b[1;32m    144\u001b[0m     )\n\u001b[1;32m    145\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_version \u001b[39m=\u001b[39m api_version \u001b[39mor\u001b[39;00m openai\u001b[39m.\u001b[39mapi_version\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/util.py:186\u001b[0m, in \u001b[0;36mdefault_api_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m openai\u001b[39m.\u001b[39mapi_key\n\u001b[1;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mraise\u001b[39;00m openai\u001b[39m.\u001b[39merror\u001b[39m.\u001b[39mAuthenticationError(\n\u001b[1;32m    187\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo API key provided. You can set your API key in code using \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key = <API-KEY>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with \u001b[39m\u001b[39m'\u001b[39m\u001b[39mopenai.api_key_path = <PATH>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m     )\n",
            "\u001b[0;31mAuthenticationError\u001b[0m: No API key provided. You can set your API key in code using 'openai.api_key = <API-KEY>', or you can set the environment variable OPENAI_API_KEY=<API-KEY>). If your API key is stored in a file, you can point the openai module at it with 'openai.api_key_path = <PATH>'. You can generate API keys in the OpenAI web interface. See https://platform.openai.com/account/api-keys for details."
          ]
        }
      ],
      "source": [
        "from langchain.tools.file_management.read import ReadFileTool\n",
        "\n",
        "# Define a function to generate llm_config from a LangChain tool\n",
        "def generate_llm_config(tool):\n",
        "    # Define the function schema based on the tool's args_schema\n",
        "    function_schema = {\n",
        "        \"name\": tool.name.lower().replace (' ', '_'),\n",
        "        \"description\": tool.description,\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {},\n",
        "            \"required\": [],\n",
        "        },\n",
        "    }\n",
        "\n",
        "    if tool.args is not None:\n",
        "      function_schema[\"parameters\"][\"properties\"] = tool.args\n",
        "\n",
        "    return function_schema\n",
        "\n",
        "# Instantiate the ReadFileTool\n",
        "read_file_tool = ReadFileTool()\n",
        "custom_tool = CircumferenceTool()\n",
        "\n",
        "# Construct the llm_config\n",
        "llm_config = {\n",
        "  #Generate functions config for the Tool\n",
        "  \"functions\":[\n",
        "      generate_llm_config(custom_tool),\n",
        "      generate_llm_config(read_file_tool),\n",
        "  ],\n",
        "  \"config_list\": config_list,  # Assuming you have this defined elsewhere\n",
        "  \"request_timeout\": 120,\n",
        "}\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    code_execution_config={\"work_dir\": \"coding\"},\n",
        ")\n",
        "\n",
        "# Register the tool and start the conversation\n",
        "user_proxy.register_function(\n",
        "    function_map={\n",
        "        custom_tool.name: custom_tool._run,\n",
        "        read_file_tool.name: read_file_tool._run,\n",
        "    }\n",
        ")\n",
        "\n",
        "chatbot = autogen.AssistantAgent(\n",
        "    name=\"chatbot\",\n",
        "    system_message=\"For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "user_proxy.initiate_chat(\n",
        "    chatbot,\n",
        "    message=\"Read the file with the path 'Test.txt', then calculate the circumference of a circle that has a radius of that files contents.\", #\" 7.81mm\" in the file\n",
        "    llm_config=llm_config,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "11cc4e60",
      "metadata": {},
      "source": [
        "# A PySpark Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y-ozf9EFCegw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-ozf9EFCegw",
        "outputId": "db7b73a8-6129-4dfb-9d5c-ac3536f310d7"
      },
      "outputs": [],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7iFp-Sm4CYq_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iFp-Sm4CYq_",
        "outputId": "2e1a2a70-53e6-4896-9232-63db6d097d63"
      },
      "outputs": [],
      "source": [
        "#Starndard Langchain example\n",
        "from langchain.agents import create_spark_sql_agent\n",
        "from langchain.agents.agent_toolkits import SparkSQLToolkit\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.utilities.spark_sql import SparkSQL\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "schema = \"langchain_example\"\n",
        "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema}\")\n",
        "spark.sql(f\"USE {schema}\")\n",
        "csv_file_path = \"./sample_data/california_housing_train.csv\"\n",
        "table = \"california_housing_train\"\n",
        "spark.read.csv(csv_file_path, header=True, inferSchema=True).write.option(\"path\", \"file:/content/spark-warehouse/langchain_example.db/california_housing_train\").mode(\"overwrite\").saveAsTable(table)\n",
        "spark.table(table).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iLtSTHoJD7Jn",
      "metadata": {
        "id": "iLtSTHoJD7Jn"
      },
      "outputs": [],
      "source": [
        "# Note, you can also connect to Spark via Spark connect. For example:\n",
        "# db = SparkSQL.from_uri(\"sc://localhost:15002\", schema=schema)\n",
        "spark_sql = SparkSQL(schema=schema)\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\")\n",
        "toolkit = SparkSQLToolkit(db=spark_sql, llm=llm)\n",
        "agent_executor = create_spark_sql_agent(llm=llm, toolkit=toolkit, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VzqNYlVjCqQa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "VzqNYlVjCqQa",
        "outputId": "dd4de772-7b0c-4650-d106-c83d4593158e"
      },
      "outputs": [],
      "source": [
        "#Starndard Langchain example\n",
        "agent_executor.run(\"Describe the california_housing_train table\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d94d45a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# LangChain direct tool usage instead of toolkit example\n",
        "# from langchain.tools.spark_sql.tool import (\n",
        "#     InfoSparkSQLTool,\n",
        "#     ListSparkSQLTool,\n",
        "#     QueryCheckerTool,\n",
        "#     QuerySparkSQLTool,\n",
        "# )\n",
        "# debug_toolkit = [\n",
        "#   QuerySparkSQLTool(db=spark_sql),\n",
        "#   InfoSparkSQLTool(db=spark_sql),\n",
        "#   ListSparkSQLTool(db=spark_sql),\n",
        "#   QueryCheckerTool(db=spark_sql, llm=llm),\n",
        "#]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r7PFvDS7Ev-E",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7PFvDS7Ev-E",
        "outputId": "53d9c45d-058e-4e37-ba73-556591aaab42"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Now use AutoGen with Langchain Tool Bridgre\n",
        "tools = []\n",
        "function_map = {}\n",
        "\n",
        "for tool in toolkit.get_tools(): #debug_toolkit if you want to use tools directly\n",
        "    tool_schema = generate_llm_config(tool)\n",
        "    print(tool_schema)\n",
        "    tools.append(tool_schema)\n",
        "    function_map[tool.name] = tool._run\n",
        "\n",
        "# Construct the llm_config\n",
        "llm_config = {\n",
        "  \"functions\": tools,\n",
        "  \"config_list\": config_list,  # Assuming you have this defined elsewhere\n",
        "  \"request_timeout\": 120,\n",
        "}\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    code_execution_config={\"work_dir\": \"coding\"},\n",
        ")\n",
        "\n",
        "print(function_map)\n",
        "\n",
        "# Register the tool and start the conversation\n",
        "user_proxy.register_function(\n",
        "    function_map = function_map\n",
        ")\n",
        "\n",
        "chatbot = autogen.AssistantAgent(\n",
        "    name=\"chatbot\",\n",
        "    system_message=\"For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "user_proxy.initiate_chat(\n",
        "    chatbot,\n",
        "    message=\"Describe the table names california_housing_train\",\n",
        "    llm_config=llm_config,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "flaml_dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
