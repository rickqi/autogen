{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/oai_chatgpt_gpt4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved. \n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# Use AutoGen to Tune ChatGPT\n",
    "\n",
    "AutoGen offers a cost-effective hyperparameter optimization technique [EcoOptiGen](https://arxiv.org/abs/2303.04673) for tuning Large Language Models. The study finds that tuning hyperparameters can significantly improve the utility of LLMs.\n",
    "Please find documentation about this feature [here](/docs/Use-Cases/AutoGen#enhanced-inference).\n",
    "\n",
    "In this notebook, we tune OpenAI ChatGPT (both GPT-3.5 and GPT-4) models for math problem solving. We use [the MATH benchmark](https://crfm.stanford.edu/helm/latest/?group=math_chain_of_thought) for measuring mathematical problem solving on competition math problems with chain-of-thoughts style reasoning.\n",
    "\n",
    "Related link: [Blogpost](https://microsoft.github.io/autogen/blog/2023/04/21/LLM-tuning-math) based on this experiment.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install with the [blendsearch] option:\n",
    "```bash\n",
    "pip install \"pyautogen[blendsearch]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.317406Z",
     "iopub.status.busy": "2023-02-13T23:40:52.316561Z",
     "iopub.status.idle": "2023-02-13T23:40:52.321193Z",
     "shell.execute_reply": "2023-02-13T23:40:52.320628Z"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install \"pyautogen[blendsearch]\" datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoGen has provided an API for hyperparameter optimization of OpenAI ChatGPT models: `autogen.ChatCompletion.tune` and to make a request with the tuned config: `autogen.ChatCompletion.create`. First, we import autogen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:54.634335Z",
     "iopub.status.busy": "2023-02-13T23:40:54.633929Z",
     "iopub.status.idle": "2023-02-13T23:40:56.105700Z",
     "shell.execute_reply": "2023-02-13T23:40:56.105085Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import autogen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set your API Endpoint\n",
    "\n",
    "The [`config_list_openai_aoai`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_openai_aoai) function tries to create a list of  Azure OpenAI endpoints and OpenAI endpoints. It assumes the api keys and api bases are stored in the corresponding environment variables or local txt files:\n",
    "\n",
    "- OpenAI API key: os.environ[\"OPENAI_API_KEY\"] or `openai_api_key_file=\"key_openai.txt\"`.\n",
    "- Azure OpenAI API key: os.environ[\"AZURE_OPENAI_API_KEY\"] or `aoai_api_key_file=\"key_aoai.txt\"`. Multiple keys can be stored, one per line.\n",
    "- Azure OpenAI API base: os.environ[\"AZURE_OPENAI_API_BASE\"] or `aoai_api_base_file=\"base_aoai.txt\"`. Multiple bases can be stored, one per line.\n",
    "\n",
    "It's OK to have only the OpenAI API key, or only the Azure OpenAI API key + base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.324240Z",
     "iopub.status.busy": "2023-02-13T23:40:52.323783Z",
     "iopub.status.idle": "2023-02-13T23:40:52.330570Z",
     "shell.execute_reply": "2023-02-13T23:40:52.329750Z"
    }
   },
   "outputs": [],
   "source": [
    "config_list = autogen.config_list_openai_aoai()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {'api_key': '<your OpenAI API key here>'},  # only if OpenAI API key is found\n",
    "    {\n",
    "        'api_key': '<your first Azure OpenAI API key here>',\n",
    "        'api_base': '<your first Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },  # only if the at least one Azure OpenAI API key is found\n",
    "    {\n",
    "        'api_key': '<your second Azure OpenAI API key here>',\n",
    "        'api_base': '<your second Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },  # only if the second Azure OpenAI API key is found\n",
    "]\n",
    "```\n",
    "\n",
    "You can directly override it if the above function returns an empty list, i.e., it doesn't find the keys in the specified locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-4\",\n",
    "            \"gpt4\",\n",
    "            \"gpt-4-32k\",\n",
    "            \"gpt-4-32k-0314\",\n",
    "            \"gpt-4-32k-v0314\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "            \"gpt-3.5-turbo-16k\",\n",
    "            \"gpt-3.5-turbo-0301\",\n",
    "            \"chatgpt-35-turbo-0301\",\n",
    "            \"gpt-35-turbo-v0301\",\n",
    "            \"gpt\",\n",
    "            \"gpt-35-turbo\",\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "We load the competition_math dataset. The dataset contains 201 \"Level 2\" Algebra examples. We use a random sample of 20 examples for tuning the generation hyperparameters and the remaining for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:52.339977Z",
     "iopub.status.busy": "2023-02-13T23:40:52.339556Z",
     "iopub.status.idle": "2023-02-13T23:40:54.603349Z",
     "shell.execute_reply": "2023-02-13T23:40:54.602630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 201\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "seed = 41\n",
    "data = datasets.load_dataset(\"competition_math\")\n",
    "train_data = data[\"train\"].shuffle(seed=seed)\n",
    "test_data = data[\"test\"].shuffle(seed=seed)\n",
    "n_tune_data = 20\n",
    "tune_data = [\n",
    "    {\n",
    "        \"problem\": train_data[x][\"problem\"],\n",
    "        \"solution\": train_data[x][\"solution\"],\n",
    "    }\n",
    "    for x in range(len(train_data)) if train_data[x][\"level\"] == \"Level 2\" and train_data[x][\"type\"] == \"Algebra\"\n",
    "][:n_tune_data]\n",
    "test_data = [\n",
    "    {\n",
    "        \"problem\": test_data[x][\"problem\"],\n",
    "        \"solution\": test_data[x][\"solution\"],\n",
    "    }\n",
    "    for x in range(len(test_data)) if test_data[x][\"level\"] == \"Level 2\" and test_data[x][\"type\"] == \"Algebra\"\n",
    "]\n",
    "print(len(tune_data), len(test_data))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check a tuning example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:54.607152Z",
     "iopub.status.busy": "2023-02-13T23:40:54.606441Z",
     "iopub.status.idle": "2023-02-13T23:40:54.610504Z",
     "shell.execute_reply": "2023-02-13T23:40:54.609759Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If $3+a=4-b$ and $4+b=7+a$, what is $3-a$?\n"
     ]
    }
   ],
   "source": [
    "print(tune_data[1][\"problem\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one example of the canonical solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:54.613590Z",
     "iopub.status.busy": "2023-02-13T23:40:54.613168Z",
     "iopub.status.idle": "2023-02-13T23:40:54.616873Z",
     "shell.execute_reply": "2023-02-13T23:40:54.616193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First we begin by solving the system of equations \\begin{align*}\n",
      "3+a&=4-b, \\\\\n",
      "4+b&=7+a.\n",
      "\\end{align*}Adding the two equations, we get $3+a+4+b=4-b+7+a$, which simplifies to $7+a+b=11+a-b$. Cancelling $a$ from both sides, we get $7+b=11-b$. Solving for $b$, we find that $b=2$. Plugging this into the first equation above, we obtain $3+a=4-2$. Hence $a=-1$ and $3-a=\\boxed{4}$.\n"
     ]
    }
   ],
   "source": [
    "print(tune_data[1][\"solution\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Success Metric\n",
    "\n",
    "Before we start tuning, we need to define the success metric we want to optimize. For each math task, we use voting to select a response with the most common answers out of all the generated responses. If it has an equivalent answer to the canonical solution, we consider the task as successfully solved. Then we can optimize the mean success rate of a collection of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:54.626998Z",
     "iopub.status.busy": "2023-02-13T23:40:54.626593Z",
     "iopub.status.idle": "2023-02-13T23:40:54.631383Z",
     "shell.execute_reply": "2023-02-13T23:40:54.630770Z"
    }
   },
   "outputs": [],
   "source": [
    "from autogen.math_utils import eval_math_responses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use the tuning data to find a good configuration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For (local) reproducibility and cost efficiency, we cache responses from OpenAI with a controllable seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:56.109177Z",
     "iopub.status.busy": "2023-02-13T23:40:56.108624Z",
     "iopub.status.idle": "2023-02-13T23:40:56.112651Z",
     "shell.execute_reply": "2023-02-13T23:40:56.112076Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "autogen.ChatCompletion.set_cache(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a disk cache in \".cache/{seed}\". You can change `cache_path_root` from \".cache\" to a different path in `set_cache()`. The cache for different seeds are stored separately.\n",
    "\n",
    "### Perform tuning\n",
    "\n",
    "The tuning will take a while to finish, depending on the optimization budget. The tuning will be performed under the specified optimization budgets.\n",
    "\n",
    "* `inference_budget` is the target average inference budget per instance in the benchmark. For example, 0.004 means the target inference budget is 0.004 dollars, which translates to 2000 tokens (input + output combined) if the gpt-3.5-turbo model is used.\n",
    "* `optimization_budget` is the total budget allowed to perform the tuning. For example, 1 means 1 dollars are allowed in total, which translates to 500K tokens for the gpt-3.5-turbo model.\n",
    "* `num_sumples` is the number of different hyperparameter configurations which is allowed to try. The tuning will stop after either num_samples trials or after optimization_budget dollars spent, whichever happens first. -1 means no hard restriction in the number of trials and the actual number is decided by `optimization_budget`.\n",
    "\n",
    "Users can specify tuning data, optimization metric, optimization mode, evaluation function, search spaces etc.. The default search space is:\n",
    "\n",
    "```python\n",
    "default_search_space = {\n",
    "    \"model\": tune.choice([\n",
    "        \"gpt-3.5-turbo\",\n",
    "        \"gpt-4\",\n",
    "    ]),\n",
    "    \"temperature_or_top_p\": tune.choice(\n",
    "        [\n",
    "            {\"temperature\": tune.uniform(0, 2)},\n",
    "            {\"top_p\": tune.uniform(0, 1)},\n",
    "        ]\n",
    "    ),\n",
    "    \"max_tokens\": tune.lograndint(50, 1000),\n",
    "    \"n\": tune.randint(1, 100),\n",
    "    \"prompt\": \"{prompt}\",\n",
    "}\n",
    "```\n",
    "\n",
    "The default search space can be overridden by users' input.\n",
    "For example, the following code specifies a fixed prompt template. For hyperparameters which don't appear in users' input, the default search space will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: optuna in /home/vscode/.local/lib/python3.10/site-packages (3.4.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/vscode/.local/lib/python3.10/site-packages (from optuna) (1.12.1)\n",
      "Requirement already satisfied: colorlog in /home/vscode/.local/lib/python3.10/site-packages (from optuna) (6.7.0)\n",
      "Requirement already satisfied: numpy in /home/vscode/.local/lib/python3.10/site-packages (from optuna) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/vscode/.local/lib/python3.10/site-packages (from optuna) (2.0.22)\n",
      "Requirement already satisfied: tqdm in /home/vscode/.local/lib/python3.10/site-packages (from optuna) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /home/vscode/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/vscode/.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/vscode/.local/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:40:56.115383Z",
     "iopub.status.busy": "2023-02-13T23:40:56.114975Z",
     "iopub.status.idle": "2023-02-13T23:41:55.045654Z",
     "shell.execute_reply": "2023-02-13T23:41:55.044973Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Optuna must be installed! Run `pip install optuna`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flaml/tune/searcher/blendsearch.py:235\u001b[0m, in \u001b[0;36mBlendSearch.__init__\u001b[0;34m(self, metric, mode, space, low_cost_partial_config, cat_hp_cost, points_to_evaluate, evaluated_rewards, time_budget_s, num_samples, resource_attr, min_resource, max_resource, reduction_factor, global_search_alg, config_constraints, metric_constraints, seed, cost_attr, cost_budget, experimental, lexico_objectives, use_incumbent_result_in_evaluation, allow_empty_config)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     \u001b[39massert\u001b[39;00m evaluated_rewards\n\u001b[1;32m    236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gs \u001b[39m=\u001b[39m GlobalSearch(\n\u001b[1;32m    237\u001b[0m         space\u001b[39m=\u001b[39mgs_space,\n\u001b[1;32m    238\u001b[0m         metric\u001b[39m=\u001b[39mmetric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         evaluated_rewards\u001b[39m=\u001b[39mevaluated_rewards,\n\u001b[1;32m    244\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m prompts \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m{problem}\u001b[39;00m\u001b[39m Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mboxed\u001b[39m\u001b[39m{{\u001b[39m\u001b[39m}}.\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m config, analysis \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mtune(\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     data\u001b[39m=\u001b[39;49mtune_data,  \u001b[39m# the data for tuning\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msuccess_vote\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# the metric to optimize\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# the optimization mode\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     eval_func\u001b[39m=\u001b[39;49meval_math_responses,  \u001b[39m# the evaluation function to return the success metrics\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# log_file_name=\"logs/math.log\",  # the log file name\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     inference_budget\u001b[39m=\u001b[39;49m\u001b[39m0.02\u001b[39;49m,  \u001b[39m# the inference budget (dollar per instance)\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     optimization_budget\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,  \u001b[39m# the optimization budget (dollar in total)\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# num_samples can further limit the number of trials for different hyperparameter configurations;\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# -1 means decided by the optimization budget only\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     num_samples\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# comment to tune both gpt-3.5-turbo and gpt-4\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     prompt\u001b[39m=\u001b[39;49mprompts,  \u001b[39m# the prompt templates to choose from\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# stop=\"###\",  # the stop sequence\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     config_list\u001b[39m=\u001b[39;49mconfig_list,  \u001b[39m# the endpoint list\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     allow_format_str_template\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# whether to allow format string template\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# logging_level=logging.INFO,  # the logging level\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bsolid-space-invention-wrxqg6qv7x3q7g/workspaces/autogen/notebook/oai_chatgpt_gpt4.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/autogen/autogen/oai/completion.py:681\u001b[0m, in \u001b[0;36mCompletion.tune\u001b[0;34m(cls, data, metric, mode, eval_func, log_file_name, inference_budget, optimization_budget, num_samples, logging_level, **config)\u001b[0m\n\u001b[1;32m    672\u001b[0m     search_alg \u001b[39m=\u001b[39m BlendSearch(\n\u001b[1;32m    673\u001b[0m         cost_attr\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    674\u001b[0m         cost_budget\u001b[39m=\u001b[39moptimization_budget,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    678\u001b[0m         points_to_evaluate\u001b[39m=\u001b[39mpoints_to_evaluate,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    680\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 681\u001b[0m     search_alg \u001b[39m=\u001b[39m BlendSearch(\n\u001b[1;32m    682\u001b[0m         cost_attr\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    683\u001b[0m         cost_budget\u001b[39m=\u001b[39;49moptimization_budget,\n\u001b[1;32m    684\u001b[0m         metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m    685\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    686\u001b[0m         space\u001b[39m=\u001b[39;49mspace,\n\u001b[1;32m    687\u001b[0m     )\n\u001b[1;32m    688\u001b[0m old_level \u001b[39m=\u001b[39m logger\u001b[39m.\u001b[39mgetEffectiveLevel()\n\u001b[1;32m    689\u001b[0m logger\u001b[39m.\u001b[39msetLevel(logging_level)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flaml/tune/searcher/blendsearch.py:246\u001b[0m, in \u001b[0;36mBlendSearch.__init__\u001b[0;34m(self, metric, mode, space, low_cost_partial_config, cat_hp_cost, points_to_evaluate, evaluated_rewards, time_budget_s, num_samples, resource_attr, min_resource, max_resource, reduction_factor, global_search_alg, config_constraints, metric_constraints, seed, cost_attr, cost_budget, experimental, lexico_objectives, use_incumbent_result_in_evaluation, allow_empty_config)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gs \u001b[39m=\u001b[39m GlobalSearch(\n\u001b[1;32m    237\u001b[0m             space\u001b[39m=\u001b[39mgs_space,\n\u001b[1;32m    238\u001b[0m             metric\u001b[39m=\u001b[39mmetric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m             evaluated_rewards\u001b[39m=\u001b[39mevaluated_rewards,\n\u001b[1;32m    244\u001b[0m         )\n\u001b[1;32m    245\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mAssertionError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m--> 246\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gs \u001b[39m=\u001b[39m GlobalSearch(\n\u001b[1;32m    247\u001b[0m             space\u001b[39m=\u001b[39;49mgs_space,\n\u001b[1;32m    248\u001b[0m             metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m    249\u001b[0m             mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    250\u001b[0m             seed\u001b[39m=\u001b[39;49mgs_seed,\n\u001b[1;32m    251\u001b[0m             sampler\u001b[39m=\u001b[39;49msampler,\n\u001b[1;32m    252\u001b[0m         )\n\u001b[1;32m    253\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gs\u001b[39m.\u001b[39mspace \u001b[39m=\u001b[39m space\n\u001b[1;32m    254\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/flaml/tune/searcher/suggestion.py:547\u001b[0m, in \u001b[0;36mOptunaSearch.__init__\u001b[0;34m(self, space, metric, mode, points_to_evaluate, sampler, seed, evaluated_rewards)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    532\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    533\u001b[0m     space: Optional[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     evaluated_rewards: Optional[List] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    546\u001b[0m ):\n\u001b[0;32m--> 547\u001b[0m     \u001b[39massert\u001b[39;00m ot \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mOptuna must be installed! Run `pip install optuna`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    548\u001b[0m     \u001b[39msuper\u001b[39m(OptunaSearch, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(metric\u001b[39m=\u001b[39mmetric, mode\u001b[39m=\u001b[39mmode)\n\u001b[1;32m    550\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(space, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m space:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Optuna must be installed! Run `pip install optuna`."
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "prompts = [\"{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in \\\\boxed{{}}.\"]\n",
    "config, analysis = autogen.ChatCompletion.tune(\n",
    "    data=tune_data,  # the data for tuning\n",
    "    metric=\"success_vote\",  # the metric to optimize\n",
    "    mode=\"max\",  # the optimization mode\n",
    "    eval_func=eval_math_responses,  # the evaluation function to return the success metrics\n",
    "    # log_file_name=\"logs/math.log\",  # the log file name\n",
    "    inference_budget=0.02,  # the inference budget (dollar per instance)\n",
    "    optimization_budget=1,  # the optimization budget (dollar in total)\n",
    "    # num_samples can further limit the number of trials for different hyperparameter configurations;\n",
    "    # -1 means decided by the optimization budget only\n",
    "    num_samples=20,\n",
    "    model=\"gpt-3.5-turbo\",  # comment to tune both gpt-3.5-turbo and gpt-4\n",
    "    prompt=prompts,  # the prompt templates to choose from\n",
    "    # stop=\"###\",  # the stop sequence\n",
    "    config_list=config_list,  # the endpoint list\n",
    "    allow_format_str_template=True,  # whether to allow format string template\n",
    "    # logging_level=logging.INFO,  # the logging level\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output tuning results\n",
    "\n",
    "After the tuning, we can print out the config and the result found by AutoGen, which uses flaml for tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:41:55.049204Z",
     "iopub.status.busy": "2023-02-13T23:41:55.048871Z",
     "iopub.status.idle": "2023-02-13T23:41:55.053284Z",
     "shell.execute_reply": "2023-02-13T23:41:55.052574Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"optimized config\", config)\n",
    "print(\"best result on tuning data\", analysis.best_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Make a request with the tuned config\n",
    "\n",
    "We can apply the tuned config on the request for an example task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:41:55.056205Z",
     "iopub.status.busy": "2023-02-13T23:41:55.055631Z",
     "iopub.status.idle": "2023-02-13T23:41:56.039259Z",
     "shell.execute_reply": "2023-02-13T23:41:56.038427Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = autogen.ChatCompletion.create(context=tune_data[1], config_list=config_list, **config)\n",
    "metric_results = eval_math_responses(autogen.ChatCompletion.extract_text(response), **tune_data[1])\n",
    "print(\"response on an example data instance:\", response)\n",
    "print(\"metric_results on the example data instance:\", metric_results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the success rate on the test data\n",
    "\n",
    "You can use `autogen.ChatCompletion.test` to evaluate the performance of an entire dataset with the tuned config. The following code will take a while (30 mins to 1 hour) to evaluate all the test data instances if uncommented and run. It will cost roughly $3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T23:41:56.042764Z",
     "iopub.status.busy": "2023-02-13T23:41:56.042086Z",
     "iopub.status.idle": "2023-02-13T23:53:05.597643Z",
     "shell.execute_reply": "2023-02-13T23:53:05.596603Z"
    }
   },
   "outputs": [],
   "source": [
    "# result = autogen.ChatCompletion.test(test_data, logging_level=logging.INFO, config_list=config_list, **config)\n",
    "# print(\"performance on test data with the tuned config:\", result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the default, untuned gpt-4 config (with the same prompt as the tuned config)? We can evaluate it and compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code will cost roughly $2 if uncommented and run.\n",
    "\n",
    "# default_config = {\"model\": 'gpt-4', \"prompt\": prompts[0], \"allow_format_str_template\": True}\n",
    "# default_result = autogen.ChatCompletion.test(test_data, config_list=config_list, **default_config)\n",
    "# print(\"performance on test data from gpt-4 with a default config:\", default_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"tuned config succeeds in {:.1f}% test cases\".format(result[\"success_vote\"] * 100))\n",
    "# print(\"untuned config succeeds in {:.1f}% test cases\".format(default_result[\"success_vote\"] * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default use of GPT-4 has a much lower accuracy. Note that the default config has a lower inference cost. What if we heuristically increase the number of responses n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following evaluation costs $3 and longer than one hour if you uncomment it and run it.\n",
    "\n",
    "# config_n2 = {\"model\": 'gpt-4', \"prompt\": prompts[0], \"n\": 2, \"allow_format_str_template\": True}\n",
    "# result_n2 = autogen.ChatCompletion.test(test_data, config_list=config_list, **config_n2)\n",
    "# print(\"performance on test data from gpt-4 with a default config and n=2:\", result_n2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference cost is doubled and matches the tuned config. But the success rate doesn't improve much. What if we further increase the number of responses n to 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following evaluation costs $8 and longer than one hour if you uncomment it and run it.\n",
    "\n",
    "# config_n5 = {\"model\": 'gpt-4', \"prompt\": prompts[0], \"n\": 5, \"allow_format_str_template\": True}\n",
    "# result_n5 = autogen.ChatCompletion.test(test_data, config_list=config_list, **config_n5)\n",
    "# print(\"performance on test data from gpt-4 with a default config and n=5:\", result_n5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the 'success_vote' metric is increased at the cost of exceeding the inference budget. But the tuned configuration has both higher 'success_vote' (91% vs. 87%) and lower average inference cost ($0.015 vs. $0.037 per instance).\n",
    "\n",
    "A developer could use AutoGen to tune the configuration to satisfy the target inference budget while maximizing the value out of it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2d910cfd2d2a4fc49fc30fbbdc5576a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "454146d0f7224f038689031002906e6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e4ae2b6f5a974fd4bafb6abb9d12ff26",
        "IPY_MODEL_577e1e3cc4db4942b0883577b3b52755",
        "IPY_MODEL_b40bdfb1ac1d4cffb7cefcb870c64d45"
       ],
       "layout": "IPY_MODEL_dc83c7bff2f241309537a8119dfc7555",
       "tabbable": null,
       "tooltip": null
      }
     },
     "577e1e3cc4db4942b0883577b3b52755": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2d910cfd2d2a4fc49fc30fbbdc5576a7",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_74a6ba0c3cbc4051be0a83e152fe1e62",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "6086462a12d54bafa59d3c4566f06cb2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "74a6ba0c3cbc4051be0a83e152fe1e62": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7d3f3d9e15894d05a4d188ff4f466554": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b40bdfb1ac1d4cffb7cefcb870c64d45": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f1355871cc6f4dd4b50d9df5af20e5c8",
       "placeholder": "​",
       "style": "IPY_MODEL_ca245376fd9f4354af6b2befe4af4466",
       "tabbable": null,
       "tooltip": null,
       "value": " 1/1 [00:00&lt;00:00, 44.69it/s]"
      }
     },
     "ca245376fd9f4354af6b2befe4af4466": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "dc83c7bff2f241309537a8119dfc7555": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e4ae2b6f5a974fd4bafb6abb9d12ff26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6086462a12d54bafa59d3c4566f06cb2",
       "placeholder": "​",
       "style": "IPY_MODEL_7d3f3d9e15894d05a4d188ff4f466554",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "f1355871cc6f4dd4b50d9df5af20e5c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
